<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Turi Create: Technical Details: CPPIPC</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Turi Create
   &#160;<span id="projectnumber">4.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('technical_details_cppipc.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Technical Details: CPPIPC </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="technical_details_cppipc_communication"></a>
Communication</h1>
<p>The CPPIPC client and server communicates with each other via libfault's ZeroMQ sanity wrappers. Specifically, the libfault::async_request_socket (client) and the libfault::async_reply_socket (server) which implements a reliable asynchronous request-reply pattern. (i.e. Each request sent must be paired with a reply. Multiple requests can be sent simultaneously).</p>
<p>Libfault's wrappers were originally designed to watch for state changes on Zookeeper and react accordingly. For instance, a collection of servers could have running processes which implement a service name called "echo" (which echos messages). The service is implemented using the async_reply_socket which registers a key called "echo" on Zookeeper, which keep tracks of the TCP/IP address of each server process. A client (with an async_reply_socket) could then connect to the "echo" service and request messages can be sent to the "service" reliably. i.e. When servers go down, they will be unregistered on Zookeeper automatically and the clients will pick up those changes and adapt accordingly. Similarly when new servers come up. The restriction to simple request-reply patterns (and pub/sub patterns) allow reliability to be provided easily (as opposed to arbitrarily interesting protocols).</p>
<p>However, for the purposes of the current Unity engine, Zookeeper is quite unnecessary since service migration/faults are not a concern. Instead we would only like the reliable communication patterns implemented. As such, libfault::reply_socket, libfault::request_socket, libfault::async_reply_socket, and libfault::async_request_socket has been modified to operate without the use of Zookeeper.</p>
<p>The Libfault interface is moderately well documented, but the internals are generally not hugely well documented since a large part of it is in working around ZeroMQ's oddities. We will try to go into a few relevant details here, but you should look at ZeroMQ for the specifics.</p>
<h2><a class="anchor" id="technical_details_cppipc_zmq_msg"></a>
zmq_msg_vector</h2>
<p>To understand the deeper parts of the Comm layer, it is uesful to get a quick brief scan at ZeroMQ, and in particular ZeroMQ's message object (zmq_msg*) in <a href="http://api.zeromq.org/4-0:_start">http://api.zeromq.org/4-0:_start</a> (for which I have a wrapper in libfault::zmq_msg_vector). The</p>
<p>ZeroMQ message object (zmq_msg_t) is basically a reference counted character buffer. It has several optimizations such as for small messages, it will handle it in-place without a malloc, and for larger messages, it will use a reference counted buffer on the heap. If you use the zmq_msg_* functions from ZeroMQ, there will be no issues with leaks and such. Only thing that you have to be careful of, is that zmq_msg_t should not be copied: i.e.</p>
<div class="fragment"><div class="line"><span class="comment">// given zmq_msg_t object msg1 and msg2</span></div><div class="line">msg1 = msg2 <span class="comment">// This is not safe!</span></div></div><!-- fragment --><p>You should always manage zmq_msg_t object as pointers.</p>
<p>The libfault::zmq_msg_vector object manages safely, an array of zmq_msg_t buffers with appropriate iteration and manipulation capabilities (most unusually pop_front, and push_back). The zmq_msg_vector is the key message object that is sent and received.</p>
<p>It is important to note that ZeroMQ provides a <b>message</b> protocol abstraction and not a <b>stream</b> protocol abstraction. i.e. if you send a zmq_msg_vector with 4 parts, the receiver will also receive a zmq_msg_vector with the same 4 parts. This makes the pop_front/push_front functionality extremely useful this this allows you to easily stack additional headers on the sender and strip them one by one on the receiver. (This is also how the DEALER-ROUTER sockets work to tag messages with their appropriate destinations. See ZeroMQ Message Envelopes).</p>
<h2><a class="anchor" id="technical_details_cppipc_request_reply"></a>
REQUEST-REPLY Pattern</h2>
<p>The REQUEST-REPLY pattern is one of the simplest ZeroMQ pattern. Basically, the Reply socket is a socket pattern where you can only perform alternating receive and send operations. And the REQUEST socket is exactly the opposite where you can only perform alternating send and receive operations. Thus the simplest pattern is where a REQUEST socket connects to a Reply socket:</p>
<ul>
<li>Request socket sends a request message</li>
<li>Request socket then waits on reply</li>
<li>Reply socket receives the request message</li>
<li>Reply socket sends the reply</li>
<li>Request socket receives the reply and this can then repeat.</li>
</ul>
<p>The cool part, is that multiple REQUEST sockets can connect to one Reply socket. The strict sequential recv/send pattern however means that the reply socket can only process one message at a time.</p>
<p>The REQUEST and REPLY socket are wrapped in libfault::request_socket and libfault::reply_socket respectively and appropriately handles situations such as message failures, malformed messages, timeouts, etc.</p>
<h2><a class="anchor" id="technical_details_cppipc_router_dealer"></a>
(DEALER-ROUTER Pattern) Async Request-Async Reply</h2>
<p>The DEALER is a generalized REQUEST socket, and the ROUTER is a generalized REPLY socket. The key difference is that the DEALER and ROUTER sockets can send and receive arbitrarily without restrictions. DEALER can also connect and load balance requests to multiple servers, though we are not using that capability.</p>
<p>The DEALER (async request) and ROUTER (async reply) sockets are wrapped in libfault::async_request_socket and libfault::async_reply_socket respectively and appropriately handles situations such as message failures, malformed messages, timeouts, etc.</p>
<p>The async_request_socket has the following internal architecture:</p><ul>
<li>When a "send" is called, the message is dropped into an inproc PULL/PUSH pair which collect messages sent from arbitrary threads to be processed by a single thread. The send then immediately returns a future.</li>
<li>The single thread (libfault::async_request_socket::pull_socket_callback) waits on the PUSH side of the inproc socket, picks up the message, and forwards it to the appropriate destination. (Reason for the pull/push to single thread thing is that in the "Zookeeper Service" case, this thread may actually need to create new DEALER sockets and it simplifies the logic substantially if the logic is centralized in one place.)</li>
<li>Received messages from the server wake up a second callback (lifault::async_request_socket::remote_message_callback) which finds the matching promise and resolves the original future</li>
</ul>
<p>The async_reply_socket has a very similar internal architecture, but in the opposite direction.</p><ul>
<li>Messages received from the ROUTER wake up a callback (libfault::async_reply_socket::wrapped_callback) which drop the message into a queue protected by a mutex/condition variable pair.)</li>
<li>A collection of handler threads waiting on the queue will wake up to wake handle the message and produce replies.</li>
<li>The replies from the handler threads are dropped into an inproc PULL/PUSH pair to collect messages into a single thread.</li>
<li>A callback waiting on the PULL socket (libfault::async_reply_socket::pull_socket_callback) forwards the messages back out through the ROUTER.</li>
</ul>
<h1><a class="anchor" id="technical_details_cppipc_object_registry"></a>
Object Registry</h1>
<p>The CPPIPC Server (<a class="el" href="classcppipc_1_1comm__server.html">cppipc::comm_server</a>) internally manages a list of object types and the knowledge of how to create instances of such objects. So when the client (<a class="el" href="classcppipc_1_1comm__client.html">cppipc::comm_client</a>) asks to create a new object of a particular type, it can be instantiated on the server.</p>
<p>Types are registered by <b>name</b> of the base class and on the server, <a class="el" href="classcppipc_1_1comm__server.html#ae1a4b406f8b0061a14e00634228ee8bd">cppipc::comm_server::register_type</a> is used to associate it with a construction function. The actual list is really managed by the object_factory (in cppipc/common/) which is in itself, a CPPIPC shared object (you will see there is a proxy, base, and impl).</p>
<dl class="section note"><dt>Note</dt><dd>This introduces an interesting chicken and egg problem: how do you register the type of the object factory in the first place? This is basically done by some manual construction and registration of the object_factory in both the server and the client. This design allows for a very simple means of proxy construction on the client side: you will observe the implementation of <a class="el" href="classcppipc_1_1comm__client.html#a0535adcf8ed155dc240a236b0033d43a">cppipc::comm_client::make_object</a> is only one line of code: directing the call to the object_factory_proxy which simply reuses the regular IPC communication channel to actually construct the object. This design means that the Client and Server do not require any special additional handling for internal coordination, but can simply use the same protocol as everything else.</dd></dl>
<h1><a class="anchor" id="technical_details_cppipc_function_registry"></a>
Function Registry</h1>
<p>In addition to a registry of types, we also need a registry of the functions that can be called. This function list is known to both the client and server and at the moment, no interesting "mangling" is performed. It is simply the full name if the function in the form &lt;typename&gt;::&lt;functionname&gt;.</p>
<dl class="section note"><dt>Note</dt><dd>In an earlier attempt of the cppipc, the function name also includes the complete C++ name mangled type signature thus allowing different overloads the same function to have completely different registrations. This will be substantially more robust since any changes in the signature will result in a completely different function call making backward compatibility much easier to achieve. However, as it turns out due to various reasons, Mac-CLang has a different name mangling scheme than Linux thus when this is used, Mac can no longer communicate with Linux servers, and vice versa.</dd></dl>
<p>The server needs the function registry to know how to convert a function name to a function pointer. The client needs the function registry to perform the inverse: conversion from a function pointer to a function name (see <a class="el" href="technical_details_cppipc.html#technical_details_cppipc_real_proxy_object">The True Proxy Object</a> to see why).</p>
<p>To make sure that both client and server are registered equivalently, the actual function registration function is implemented in the exported base class, as a <b>register</b> function. See the <a class="el" href="group__cppipc.html#gab991ba71aa82c1aa330ae2054ea41baf">REGISTRATION_BEGIN</a> and REGISTER macros. They basically implement a templated function of the form:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Registry&gt;</div><div class="line"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keywordtype">void</span> __register__(Registry&amp; reg) {</div><div class="line">  reg.register_function(&amp;basic_counter_base::add, <span class="stringliteral">&quot;basic_counter_base::add&quot;</span>);</div><div class="line">  reg.register_function(&amp;basic_counter_base::add_multiple, <span class="stringliteral">&quot;basic_counter_base::add_multiple&quot;</span>);</div><div class="line">}</div></div><!-- fragment --><p>Then when the type is registered on the comm_server, it simply calls:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</div><div class="line"><span class="keywordtype">void</span> register_type(std::function&lt;T*()&gt; constructor_call) {</div><div class="line"> T::__register__(*<span class="keyword">this</span>);</div><div class="line"> ...</div><div class="line">}</div></div><!-- fragment --><p> to get all the member functions.</p>
<p>On the client side, the function registration happens on construction of the object_proxy in pretty much the same way.</p>
<h1><a class="anchor" id="technical_details_cppipc_real_proxy_object"></a>
The True Proxy Object</h1>
<p>The proxy object generated by GENERATE_INTERFACE_AND_PROXY is not the true proxy object, but is really only a light-weight wrapper around <a class="el" href="classcppipc_1_1object__proxy.html">cppipc::object_proxy</a> which actually implements the call logic. <a class="el" href="classcppipc_1_1object__proxy.html">cppipc::object_proxy</a> is a general purpose proxy which can wrap any interface class. For instance: in the basic_counter example in <a class="el" href="using_cppipc_basics.html#using_cppipc_client_and_server">Implementing a Server and a Client Program</a>, instead of using the basic_counter_proxy in the client example, I could equivalently write:</p>
<div class="fragment"><div class="line"><span class="comment">// Client Example</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;core/system/cppipc/cppipc.hpp&gt;</span></div><div class="line"><span class="preprocessor">#include &quot;basic_counter.hpp&quot;</span></div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span>** argv) {</div><div class="line">  <span class="comment">// Connects to the server</span></div><div class="line">  <a class="code" href="classcppipc_1_1comm__client.html">cppipc::comm_client</a> client({}, <span class="stringliteral">&quot;ipc:///tmp/cppipc_server_test&quot;</span>);</div><div class="line"></div><div class="line">  <span class="comment">// Creates a proxy object. This calls the factory on the server to create</span></div><div class="line">  <span class="comment">// a basic_counter on the server. The proxy object on the client only</span></div><div class="line">  <span class="comment">// maintains an object ID.</span></div><div class="line">  object_proxy&lt;basic_counter_base&gt; proxy(client);</div><div class="line"></div><div class="line">  <span class="comment">//adds 50 to the counter</span></div><div class="line">  proxy.call(&amp;basic_counter_base::add, 50);</div><div class="line"></div><div class="line">  <span class="comment">//adds 12 * 5 to the counter</span></div><div class="line">  proxy.call(&amp;basic_counter_base::add_multiple, 12, 5);</div><div class="line"></div><div class="line">  <span class="comment">// prints the counter value</span></div><div class="line">  std::cout &lt;&lt; <span class="stringliteral">&quot;Counter Value: &quot;</span> &lt;&lt; proxy.call(&amp;basic_counter_base::get_val) &lt;&lt; <span class="stringliteral">&quot;\n&quot;</span>;</div><div class="line"></div><div class="line">  <span class="comment">// when proxy is destroyed, it destroys the object on the server</span></div><div class="line">}</div></div><!-- fragment --><p>Now, one might ask why does proxy.call take a function pointer as the first argument and not simply the function name? Why not:</p>
<div class="fragment"><div class="line">proxy.call(<span class="stringliteral">&quot;basic_counter_base::add&quot;</span>, 50);</div></div><!-- fragment --><p>in which case, the client side does not need a function registry (<a class="el" href="technical_details_cppipc.html#technical_details_cppipc_function_registry">Function Registry</a>) The reason is that by using the function pointer, I can fully type check the argument types at compile time, and automatically cast to the types the server is expecting. This simplifies the process of exporting objects substantially since we do not need a run-time "schema". Essentially the "schema" is defined entirely by the type of the function, and we can rely on C++ typing to enforce the type signature.</p>
<h1><a class="anchor" id="technical_details_cppipc_serialiation"></a>
Basic Call Serialization/Deserialization</h1>
<p>The basic call serialization/deserialization code is in <a class="el" href="issue_8hpp_source.html">core/system/cppipc/client/issue.hpp</a> and <a class="el" href="dispatch__impl_8hpp_source.html">core/system/cppipc/server/dispatch_impl.hpp</a> respectively. The call/issue (client) side is surprisingly simple.</p>
<p>The actual <a class="el" href="group__cppipc.html#ga383d0e8ee303fa3678cda7d6d8fa51e7">issue</a> call is a variadic template function which tries to serialize the arguments of the call into the archive. The trick is that to ensure that types match up on the client side, it is important to cast the argument type right now (thus we need to know the Member Function pointer as well).</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> MemFn, <span class="keyword">typename</span>... Args&gt;</div><div class="line"><span class="keywordtype">void</span> <a class="code" href="group__cppipc.html#ga383d0e8ee303fa3678cda7d6d8fa51e7">issue</a>(<a class="code" href="classturi_1_1oarchive.html">turi::oarchive</a>&amp; msg,</div><div class="line">           MemFn fn,</div><div class="line">           <span class="keyword">const</span> Args&amp;... args);</div></div><!-- fragment --><p>The issue function basically takes the argument types of fn, converting it to a tuple over the argument types (<a class="el" href="structcppipc_1_1function__args__to__tuple.html">cppipc::function_args_to_tuple</a>), which then calls into <a class="el" href="structcppipc_1_1detail_1_1issue__disect.html">cppipc::detail::issue_disect</a> which basically extracts of the left most argument, and the left most entry of the tuple, cast the argument to the appropriate type, serializing it, and calling <a class="el" href="structcppipc_1_1detail_1_1issue__disect.html">cppipc::detail::issue_disect</a> tail recursively until we run out of arguments.</p>
<p>The dispatch side is slightly more complicated since it most both deserialize, call the target function, and serialize the result. But it basically operates on the same principle. cppipc::execute_disect takes a tuple of the argument types and a list of arguments deserialized so far. It then extracts the left most entry from the tuple and deserializes it, and tail recurses until it runs out of arguments.</p>
<h1><a class="anchor" id="technical_details_cppipc_object_serialization"></a>
Serializing Object Pointers</h1>
<p>Now, how about the "magic-trick" involving the ability to serialize pointers to shared objects and have then resurrect appropriately as either pointer to proxy objects (on the client) or pointer to implementation objects (on the server)? See <a class="el" href="using_cppipc_advanced_object_creation.html">Advanced Object Creation</a>.</p>
<p>This is accomplished very simply by hacking the serialization library. It is useful to understand the <a class="el" href="group__technical__details__serialization.html">serialization library technical details</a> first.</p>
<p>The source for the serialization "hack" is in <a class="el" href="ipc__deserializer_8hpp_source.html">ipc_deserializer.hpp</a>/ipc_deserializer.cpp. We will walk through this slowly.</p>
<p>Firstly, we want to catch every attempt to serialize/deserialize exported objects. To do this we must be able to identify these objects, and the easiest way to do so is to have them inherit from a common base class, and that will be cppipc::ipc_object_base which is simply an empty base class. The GENERATE_INTERFACE_AND_PROXY will automatically have the interface class inherit from cppipc::ipc_object_base thus allowing all descendents (proxy and implementation) to all be descendents of cppipc::ipc_object_base.</p>
<p>Next, we perform a partial specialization of the serialization classes serialize_impl and deserialize_impl (see <a class="el" href="group__technical__details__serialization.html">Technical Details: Serialization</a> for details). Note that we are going to intercept attempts at serializing <b>pointers</b> to the proxy/implementation classes. The enable_if line basically means that the code exists if and only if T* is convertible to cppipc::ipc_object_base*, i.e. T inherits from ipc_object_base. As a result attempts to serialize other regular pointers (which is an unsafe operation anyway), will not hit the following code.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> OutArcType, <span class="keyword">typename</span> T&gt;</div><div class="line"><span class="keyword">struct </span>serialize_impl&lt;OutArcType, T*, <span class="keyword">true</span>&gt; {</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">static</span></div><div class="line">  <span class="keyword">typename</span> std::enable_if&lt;std::is_convertible&lt;T*, cppipc::ipc_object_base*&gt;::value&gt;::type</div><div class="line">  exec(OutArcType&amp; oarc, <span class="keyword">const</span> T* value) {</div><div class="line">    ...</div><div class="line">  }</div><div class="line">};</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> InArcType, <span class="keyword">typename</span> T&gt;</div><div class="line"><span class="keyword">struct </span>deserialize_impl&lt;InArcType, T*, <span class="keyword">true</span>&gt; {</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">static</span></div><div class="line">  <span class="keyword">typename</span> std::enable_if&lt;std::is_convertible&lt;T*, cppipc::ipc_object_base*&gt;::value&gt;::type</div><div class="line">  exec(InArcType&amp; iarc, T*&amp; value) {</div><div class="line">    ...</div><div class="line">  }</div><div class="line">};</div></div><!-- fragment --><p>Now, the key annoyance is that this code is the same on both the server and the client, so I have to know whether I am on the server side, or the client side and act appropriately. To do that, I rely on a collection of two functions, which are called by the <a class="el" href="classcppipc_1_1comm__client.html">cppipc::comm_client</a> and <a class="el" href="classcppipc_1_1comm__server.html">cppipc::comm_server</a> immediately before attempting to serialize/deserialize a call. </p><div class="fragment"><div class="line"><span class="keyword">extern</span> <span class="keywordtype">void</span> set_deserializer_to_server(comm_server* server);</div><div class="line"><span class="keyword">extern</span> <span class="keywordtype">void</span> set_deserializer_to_client(comm_client* client);</div></div><!-- fragment --><p> In the implementation of these (in ipc_deserializer.cpp), they basically set a thread local variable. This allows the functions above to be fully thread safe, and allows both server/client to reside on the same machine in different threads if necessary.</p>
<p>The serialize_impl and deserialize_impl structs can then use </p><div class="fragment"><div class="line"><span class="keyword">extern</span> <span class="keywordtype">void</span> get_deserialization_type(comm_server** server, comm_client** client);</div></div><!-- fragment --><p> to figure out whether it is currently working on the server-side or the client-side.</p>
<h2><a class="anchor" id="technical_details_cppipc_object_serialization_serializing"></a>
Serializing Object Pointers-Serializing</h2>
<p>Serialization is simple, </p><div class="fragment"><div class="line"><span class="keywordflow">if</span> (server) {</div><div class="line">  <span class="comment">// server to client message is a pair of &quot;is new object&quot; and &quot;object ID&quot;</span></div><div class="line">  oarc &lt;&lt; get_server_object_id(server, value);</div><div class="line">} <span class="keywordflow">else</span> {</div><div class="line">  oarc &lt;&lt; (*value);</div><div class="line">}</div></div><!-- fragment --><p>The proxy object has a built in save/load function that simply serializes the object ID. For an impl object however, I will need to ask the comm_server to find the object ID.</p>
<h2><a class="anchor" id="technical_details_cppipc_object_serialization_deserializing"></a>
Serializing Object Pointers-Deserializing</h2>
<p>Deserializing is slightly more involving. On the server-side, I have to search on the comm_server object for the object matching the object ID, </p><div class="fragment"><div class="line"><span class="keywordflow">if</span> (server) {</div><div class="line">  <span class="keywordtype">size_t</span> object_id;</div><div class="line">  iarc &gt;&gt; object_id;</div><div class="line">  <span class="keywordtype">void</span>* obj = cppipc::detail::get_server_object_ptr(server, object_id);</div><div class="line">  <span class="keywordflow">if</span> (obj == NULL) {</div><div class="line">    <span class="keywordflow">throw</span> std::to_string(object_id) + <span class="stringliteral">&quot; Object not found&quot;</span>;</div><div class="line">  }</div><div class="line">  value = <span class="keyword">reinterpret_cast&lt;</span>T*<span class="keyword">&gt;</span>(obj);</div><div class="line">}</div></div><!-- fragment --><p>On the client side, the proxy object is constructed with the object_id received. (The proxy_object's class name is always typedef'd to 'proxy_object_type') </p><div class="fragment"><div class="line"><span class="keywordtype">size_t</span> object_id;</div><div class="line">iarc &gt;&gt; object_id;</div><div class="line">value = <span class="keyword">new</span> <span class="keyword">typename</span></div><div class="line">  std::remove_pointer&lt;T&gt;::type::proxy_object_type(*client, <span class="keyword">false</span>,</div><div class="line">                                                  object_id);</div></div><!-- fragment --><h1><a class="anchor" id="technical_details_cppipc_limitations"></a>
Limitations</h1>
<p>The current serializer/deserializer was designed for performance in mind and is meant to be used between "trusted" and equivalent systems. We do not have "type-aware" or "robust" serialization/deserialization. We rely heavily on the client and server agreeing on the function argument types, and having serialization behave the same way. If there are any malformed messages, or if for whatever reason the client/server function types disagree, the serializer will crash and burn. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="using_cppipc.html">Introduction to CPPIPC</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
